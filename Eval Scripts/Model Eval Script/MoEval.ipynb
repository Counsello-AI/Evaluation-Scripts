{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420ea265",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install boto3 torch transformers tqdm SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d322b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edaf945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_files_from_s3(bucket_name, prefix, local_directory):\n",
    "    session = boto3.Session()\n",
    "    s3 = session.client('s3')\n",
    "    \n",
    "    if not os.path.exists(local_directory):\n",
    "        os.makedirs(local_directory)\n",
    "    \n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "        for file in page.get('Contents', []):\n",
    "            file_name = file['Key']\n",
    "            local_path = os.path.join(local_directory, os.path.relpath(file_name, prefix))\n",
    "            \n",
    "            local_file_directory = os.path.dirname(local_path)\n",
    "            if not os.path.exists(local_file_directory):\n",
    "                os.makedirs(local_file_directory)\n",
    "            \n",
    "            print(f\"Downloading {file_name}...\")\n",
    "            s3.download_file(bucket_name, file_name, local_path)\n",
    "            print(f\"Saved to {local_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576e54f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_questions(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read().strip().split('\\n')\n",
    "    \n",
    "    questions = {}\n",
    "    current_question = None\n",
    "    current_options = {}\n",
    "    \n",
    "    for line in content:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        if line[0].isdigit():\n",
    "            if current_question:\n",
    "                questions[current_question['number']] = current_question\n",
    "            \n",
    "            question_number = int(line.split('.')[0])\n",
    "            question_text = line\n",
    "            current_question = {'number': question_number, 'question': question_text, 'options': {}}\n",
    "            current_options = {}\n",
    "        elif line[0] == '(' and ')' in line:\n",
    "            option_letter, option_text = line.split(')', 1)\n",
    "            option_letter = option_letter.strip('(').strip()\n",
    "            option_text = option_text.strip()\n",
    "            current_options[option_letter] = option_text\n",
    "            current_question['options'] = current_options\n",
    "    \n",
    "    if current_question:\n",
    "        questions[current_question['number']] = current_question\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e5212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_answer_key(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return {int(line.split('-')[0].strip()): line.split('-')[1].strip() for line in file}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780eea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model_directory):\n",
    "    print(f\"Loading model from {model_directory}\")\n",
    "    print(\"Contents of model directory:\")\n",
    "    for file in os.listdir(model_directory):\n",
    "        print(f\"  {file}\")\n",
    "    \n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_directory)\n",
    "        print(\"Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "        print(\"Tokenizer loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5077b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_predictions(model, tokenizer, questions, device):\n",
    "    predictions = {}\n",
    "    batch_size = 1  # Process one question at a time to minimize memory usage\n",
    "\n",
    "    try:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        for q_num, q_data in tqdm(questions.items(), desc=\"Generating predictions\"):\n",
    "            input_text = f\"{q_data['question']}\\n\"\n",
    "            for option, text in q_data['options'].items():\n",
    "                input_text += f\"{option}) {text}\\n\"\n",
    "            input_text += \"The correct answer is:\"\n",
    "            \n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            \n",
    "            try:\n",
    "                inputs = inputs.to(device)\n",
    "                \n",
    "                with torch.no_grad(), autocast(enabled=True):\n",
    "                    outputs = model.generate(**inputs, max_new_tokens=5, num_return_sequences=1)\n",
    "                \n",
    "                predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                predicted_answer = predicted_text.split(\"The correct answer is:\")[-1].strip()[0]\n",
    "                \n",
    "                predictions[q_num] = predicted_answer\n",
    "                \n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(f\"WARNING: GPU OOM for question {q_num}. Falling back to CPU.\")\n",
    "\n",
    "                    model.to('cpu')\n",
    "                    inputs = inputs.to('cpu')\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = model.generate(**inputs, max_new_tokens=5, num_return_sequences=1)\n",
    "                    \n",
    "                    predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                    predicted_answer = predicted_text.split(\"The correct answer is:\")[-1].strip()[0]\n",
    "                    \n",
    "                    predictions[q_num] = predicted_answer\n",
    "                    \n",
    "                    model.to(device)\n",
    "                else:\n",
    "                    raise e\n",
    "            \n",
    "            gc.collect()\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during prediction: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ed8e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, answer_key):\n",
    "    correct_count = 0\n",
    "    for q_num, pred in predictions.items():\n",
    "        correct_ans = answer_key.get(q_num)\n",
    "        if pred.strip().upper() == correct_ans.strip().upper():\n",
    "            correct_count += 1\n",
    "            print(f\"Question {q_num}: Correct (Predicted: {pred}, Correct: {correct_ans})\")\n",
    "        else:\n",
    "            print(f\"Question {q_num}: Incorrect (Predicted: {pred}, Correct: {correct_ans})\")\n",
    "    accuracy = correct_count / len(predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c37e44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket_name = \"sagemaker-us-east-1-637423474134\"\n",
    "prefix = \"k-llama3-8b-fullds-lrcos-r32-q-2024-07-04-22-11-56-336/output/model/\"\n",
    "    \n",
    "local_directory = \"/tmp/downloaded_model\"\n",
    "model_directory = local_directory  # The model will be loaded from where it's downloaded\n",
    "\n",
    "download_files_from_s3(bucket_name, prefix, local_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee9e09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    model, tokenizer = load_model(model_directory)\n",
    "    \n",
    "    try:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            torch.cuda.set_per_process_memory_fraction(0.9)\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        questions = read_questions(\"./AIBE-18-B.txt\")\n",
    "        print(f\"Successfully read {len(questions)} questions\")\n",
    "        if len(questions) == 0:\n",
    "            raise ValueError(\"No questions were read from the file\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading questions: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        answer_key = read_answer_key(\"./AIBE-18-B_Answer-Key.txt\")\n",
    "        print(f\"Successfully read {len(answer_key)} answers\")\n",
    "        if len(answer_key) == 0:\n",
    "            raise ValueError(\"No answers were read from the file\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading answer key: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    predictions = get_model_predictions(model, tokenizer, questions, device)\n",
    "\n",
    "    accuracy = calculate_accuracy(predictions, answer_key)\n",
    "    print(f\"Model Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a90656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf486bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
